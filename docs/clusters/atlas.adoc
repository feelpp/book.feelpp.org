= Atlas cluster
// Don't put authors here! (see CONTRIBUTING)
//*******************************************
:page-root: ../../
:page-layout: manual
:page-permalink: /clusters/atlas/
:doctype: book
:docinfo: shared
:toc: left
:toclevels: 5
:sectnums:
:imagesdir: ../images/
// Keep this include header at the end! (see CONTRIBUTING)
include::../includes/header.adoc[]
//*******************************************


include::../includes/topnote.adoc[]

'''
image::clusters/atlas.jpg[role="related thumb right"]
The proper ressources cluster of the 
link:http://irma.math.unistra.fr/[Research Institute Advanced Mathematics (IRMA - UMR7501)].

<<<

== Prerequisites

* You have to own an account on the machine. External user can ask for an account
if they are related to an institute research project.
* To learn how to use the cluster, we recommend reading the
link:https://gitlab.math.unistra.fr/atlas/cluster-doc/wikis/home[official documentation]
* To use differents libraries version that match your need, the cluster uses link:http://modules.sourceforge.net/[environment modules]
You should familiarize first to know how to load specific softwares.
* The cluster use link:https://slurm.schedmd.com/quickstart.html[slurm] job supervisor.
You should be familiar with job creation and job submission before going further.

TIP: We provide on the cluster Feel++ profile modules which loads all Feel++ dependencies
and that are guaranty to works!
you can type `module avail 2>&1 | grep profile` to list all profiles.

== Atlas {feelpp} configuration

The cluster mount points with different filesystems with more or less fast read/write
operations.

.Atlas main mount points
[width="100%",frame="topbot",options="header,footer"]
|===
| Mountpoint | Description | Usage

| /data/, 
| Normal I/O operations (Hard Drive),
| Store your computed data

| /ssd/,
| Fast I/O operations (Solid state drives),
| Compute

| /home/,
| Slow I/O operations (NFS file system),
| Network stored data (documents)
|===

Set the *FEELPP_WORKDIR* environment variable to the working directory path.
By default, this variable point to

[source,bash]
----
export FEELPP_WORKDIR=/ssd/${USER}/feel
----

If your application generate an high amount of data, you should use `/data/${USER}/feel` instead.

== {feelpp} usage

WARNING: Work in progress

In this section, we will learn basic usage of Feel++ on the cluster. We
distinguish two cases, the first one uses the existing Feel++ module. You might
want to modify the Feel++ library itself, the second case will explain how to
proceed.

There are three main methods to use Feel++ from the simplest to the hardest:

1. Using containers: singularity (recommended), docker.
2. Compiling a {feelpp} application using {feelpp} library installed on the
   system ({feelpp} atlas module).
3. Compiling Feel++ from scratch (library + applications).

But first, we have to configure the cluster

=== {feelpp} containers

==== Singularity (recommended)

INFO: Difficulty : easy

Singularity is the recommended method for {feelpp} containers.
Please read singularity section before going further.

To use singularity on the cluster, load the specific module:

[source,bash]
----
module load singularity/2.2.1
----

We provide currently one singularity image container in the directory `/data/images`.

TIP: Contrary to docker, singularity does not allow to modify a container 
for security purposes, root priviledges are required.

First of all, copy the image in your `${HOME}` directory

[source,bash]
----
cp /data/images/feelpp-toolboxes.img \${HOME}
----

:INFO: {feelpp} singularity images will be available online soon!

To run the container, run the following command

[source,bash]
----
singularity shell -c -B \${FEELPP_WORKDIR}/feel:/feel \${HOME}/feelpp-toolboxes.img
----

Now you are in the container, you should have the bash prompt looking like this:

image::shell/singularity/singularity-prompt1.png[align=center]


IMPORTANT: Always use the `-c` option (equivalent to `-B /home/user:/home/user`) ?
to avoid sharing your home directory within the container!
Security reason, a container can perform some operation on home directory. Configurations
files on the host `/home/user` directory can create conflicts and produce unwanted behaviour.

===== Modify the Feel++ singularity image

Root priviledge are required to modify an image. To proceed, you should first
install singularity on your machine. 
On your machine, copy a {feelpp} image locally
[source,bash]
----
scp user@irma-atlas:/data/images/feelpp-toolboxes.img ${HOME}
----
Now you can modify the image in root enabling write access `-w`

[source,bash]
----
sudo singularity shell -c -w \${HOME}/feelpp-toolboxes.img
----

See singularity documentation for more details.

===== Job slurm

An example of slurm batch script using singularity.

.myjob.slurm
[source,bash]
----
#! /bin/bash
#SBATCH -p public
#SBATCH --export=ALL
#SBATCH -n 8
#SBATCH -N 1
#SBATCH -t 00:30:00
#SBATCH -D /data/scratch/${USER}/slurm
#SBATCH -o slurm.%j.%N.out
#SBATCH -e slurm.%j.%N.err
#SBATCH -v

export FEELPP_SINGULARITY_IMAGE=${HOME}/feelpp-toolboxes.img
export FEELPP_WORKDIR_HOST=/ssd/${USER}/feel

source /etc/profile.d/modules.sh

module load singularity/2.2.1

mpirun --bind-to core \
    singularity exec \
    -B ${FEELPP_WORKDIR_HOST}:/feel \
    -c \
    ${FEELPP_SINGULARITY_IMAGE} \
    /usr/local/bin/feelpp_qs_laplacian_2d \
    --config-file=/opt/feelpp/Testcases/quickstart/laplacian/feelpp2d/feelpp2d.cfg
----

To send the job, just run the command

[source,bash]
----
sbatch myjob.slurm
----

To view your job
[source,bash]
----
squeue -u ${USER}
----

For more info about slurm, consult link:https://gitlab.math.unistra.fr/atlas/cluster-doc/wikis/slurm[atlas wiki slurm]
or link:https://slurm.schedmd.com/quickstart.html[official slurm documentation].

IMPORTANT: Multi-node is not supported yet!

==== Docker

:INFO: *Difficulty : easy*

Due to security issues, only whitelisted users can have access to it.
Please contact atlas admins if docker is required.

Refer to {feelpp} docker section to see how to use docker.

=== Feel++ via modules

:INFO: *Difficulty : intermediate*

This way is recommended if you only need to use the {feelpp} library
without any modification.
Currently, a {feelpp} module is provided for a nightly build version.

- Load the {feelpp} library via the module.

[source,sh]
----
module load feelpp/nightly
----

{feel++} is now available in the system 


TIP: the module set the environement variable *FEELPP_DIR* for the path to the {feelpp} installation
`echo ${FEELPP_DIR}`.

- Load a feel++ profile to have access to all {feelpp} requirements for compiling
your application.

[source,sh]
----
module load feelpp-clang_gcc610.profile
----

NOTE: The profile should be the same used for the feel++ module to avoid
mismatch setup for the compilation

TIP: From here you can go to Quickstart section.

- Place yourself in a directory and create a cmake file named
`CMakeLists.txt` containing the following code.

[source,cmake]
----
cmake_minimum_required(VERSION 2.8)

find_package(Feel++
  PATHS $ENV{FEELPP_DIR}/share/feel/cmake/modules
  /usr/share/feel/cmake/modules
  /usr/local/share/feel/cmake/modules
  /opt/share/feel/cmake/modules
  )
if(NOT FEELPP_FOUND)
  message(FATAL_ERROR "Feel++ was not found on your system. Make sure to install it and specify the FEELPP_DIR to reference the installation directory.")
endif()

feelpp_add_application(youApplication SRCS yourCode.cpp)
----

This file describes to cmake how to find the Feel++ library in the given directories.
By default, cmake will search in the system default path.


4. Create a {cpp} file named `yourCode.cpp`  where you will write your first Feel++ code.

5. Generate the Makefiles with cmake or using the configure script.
`/path/to/feelpp-sources/configure`

6. Compile you {feelpp} application `make`


=== Feel++ from scratch

:INFO: *Difficulty : Hard*




== Post-processing

=== Paraview

==== Downloading the data

You can retrieve your data on you local machine using rsync.

[source,bash]
----
paraview
----

and open the `.case` file


[[pvserver]]
==== Distant connection (pvserver)

Open a terminal and connect to atlas server. The paraview
Run the paraview server
[source,bash]
----
module load paraview/5.1.0
pvserver
----

TIP: use `pvserver --multi-clients` to connect with several users at the same
time! See `pvserver --help` for all options.

On you machine, run paraview and connect to the server
`[file]->[connect]`.
A should pop out. Configure the server
with the given address displayed in the terminal
where you run `pvserver`. It should looks like `cs://irma-atlas:11111`.

IMPORTANT: Paraview and pvserver must be the same version!






=== Containers



[[postprocessdocker]]
==== Docker containers

{feelpp} provides paraview directly in the docker image.
First, you have to run a new container on atlas. 
Don't forget to mount the volume `/feel` for {feelpp} applications.
We have to add an option to use the same network than irma-atlas
`--network=host`

[source,bash]
----
docker run --rm -it --network=host -v ${HOME}/feel:/feel feelpp/feelpp-toolboxes:develop-ubuntu-16.10
----

Then you can proceed with paraview server `pvserver` like in <<pvserver>>
section.

[[insitudocker]]
===== Distant connection (In-situ)

{feelpp} is compatible with insitu thanks to link:http://www.paraview.org/in-situ/[kitware catalyst].
First be sure to follow previous steps <<postprocessdocker>> to run a docker container.

In the container, start a paraview server in the background.
[source,bash]
----
pvserver &
----
Keep in mind the printed address.

On you machine, run paraview and connect to this server like explained in section
<<pvserver>>.
Connect to catalyst via paraview menu `[catalyst]->[connect]`. A prompt message
should inform you that paraview accept catalyst connections.
Set catalyst to pause `[catalyst]->[pause]` in the menu before launching a {feelpp}
application.

Now in the container, chose an application you wish to execute. For example,
let's take `~/Testcases/FSI/wavepressure2d` Run your application with insitu
options.

[source,bash]
----
mpirun -np 8 feelpp_toolbox_fsi_2d --config-file wavepressure2d.cfg --exporter.format=vtk --exporter.vtk.insitu.enable=1
----

:INFO: You have to use the VTK exporter to do insitu post-processing!

The simulation will stop after the initialisation step waiting for you to
resume catalyst.

:TIP: In Paraview, a new object should have appeared in the "pipeline browser" window.
Click on the icon to expand the object, then click on the "eye" icon to make the
object visible.

At this time, you can add all filters as usual for post-processing.

Once you're ready, resume catalyst `[catalyst]->[continue]`


Enjoy!


'''
image::logos/svg/logoIRMA.svg[cemosis,80,float="left"]
image::logos/logo_cemosis.png[cemosis,80,float="left"]
image::logos/svg/logoUDS.svg[cemosis,80,float="left"]
