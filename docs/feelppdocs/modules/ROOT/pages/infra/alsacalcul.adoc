= Alsacalcul


include::{partialsdir}/topnote.adoc[]

image::clusters/mesostra.jpg[Mesocenter of Strasbourg,float="right",align="center"]

Alsacalcul is the supercomputer of Universit√© de Strasbourg.

NOTE: For more information see the official web page of link:https://hpc.unistra.fr/[Alsacalcul].

== Prerequisites

* You have to own an account on the machine. See the
link:https://services-numeriques.unistra.fr/les-services-aux-usagers/hpc/acces-aux-ressources.html[cluster documentation]
for the first step on the cluster.
* To learn how to use the cluster, we recommend reading the
link:https://services-numeriques.unistra.fr/les-services-aux-usagers/hpc/acces-aux-ressources.html[cluster documentation]
* To use different libraries version that match your need, the cluster uses link:http://modules.sourceforge.net/[environment modules]
Learning the basics of environment modules is recommanded in order to know how to load specific softwares.
* The cluster uses link:https://slurm.schedmd.com/quickstart.html[slurm] job supervisor.
You should be familiar with job creation and job submission before going further.

== Feel++ usage

=== Using singularity container

Singularity is officially ready on the cluster.
The document is here https://hpc.pages.unistra.fr/singularity/singularity.html

[source,shell]
----
load module singularity/singularity    <1>
mpirun singularity run <feelpp image>  <2>
----
<1> load the singularity module
<2> use singularity to run the feelpp image

The document of feelpp singularity images is here xref:user:install:containers.adoc[Feel++ singularity container].

NOTE: To use the container on multi-nodes, be aware that performance may depend on network interfaces.
In particular, pay attention to `infiniband` and `omnipath` (the drivers are already packaged in our containers).


[IMPORTANT]
====
The cluster use different network communication standard to interoperate between computing nodes. Two main high performance solution are used link:https://en.wikipedia.org/wiki/InfiniBand[Infiniband] (Mellanox) and link:https://en.wikipedia.org/wiki/Omni-Path[Omnipath] (intel).
Due to proprietary reason, the current Feel++ images provide only infiniband driver, therefore the unistra cluster gives the possibility to add constraints in order to only use computing nodes that are either infiniband or omnipath interconnected only. To obtain high performances with {feelpp} singularity containers, you slurm script must contain the following constraint:
[source]
----
# omnipath: opa ; infiniband: ib
#SBATCH --constraint=ib
----
====


NOTE: Then see an example of slurm job based on source and singularity container.

=== From source

WARNING: Work in progress

To compile Feel++ on the cluster, you have to load the following required modules:

[source,bash]
----
module purge
module load batch/slurm
module load cmake/cmake-3.4.1
module load libs/boost-1.63.gnu63
module load libs/hdf5-1.8.16.gnu63
module load petsc/petsc-3.8.4.g63
module load mesh/gmsh-3.0.1.g63
module load compilers/intel17
module load compilers/clang5
module load mpi/openmpi-2.0.2.gcc63
module load languages/python-3.5.0
----

NOTE: If Feel++ does not compile using these modules, please post an
link:https://github.com/feelpp/book.feelpp.org/issues[issue on github].

An example of a cmake command to use :
[source,bash]
----
cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_COMPILER=clang++ -DCMAKE_C_COMPILER=clang -DFEELPP_ENABLE_FFTW=OFF <path to feelpp>
----

NOTE: Then see an example of a slurm job based on source and singularity container.

== Visualization

To visualize your results, you can use a visualiztion node, see link:https://services-numeriques.unistra.fr/les-services-aux-usagers/hpc/applications-disponibles/visualisation-a-distance-avec-x2go.html[X2GO documentation].
